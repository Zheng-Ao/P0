{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKdAJ8l5F_yg"
      },
      "source": [
        "# library install & cuda device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CR_-xEl5Emv2"
      },
      "outputs": [],
      "source": [
        "# !python -c \"import torch; print(torch.__version__)\"\n",
        "# !python -c \"import torch; print(torch.version.cuda)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_Zxt-SIFo9U"
      },
      "outputs": [],
      "source": [
        "# %pip install transformers\n",
        "# !pip install torch-scatter -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
        "# !pip install torch-sparse -f https://data.pyg.org/whl/torch-1.12.0+cu113.html\n",
        "# !pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCiAU3vwGSlo"
      },
      "outputs": [],
      "source": [
        "# !nvidia-smi\n",
        "import torch\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_Rn1K8pE3Z0"
      },
      "source": [
        "# utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjwGItGlE5Eh"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# from transformers import DistilBertTokenizer, DistilBertModel\n",
        "# tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")      \n",
        "# nlp_model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
        "\n",
        "'''加载BERT这种大模型很耗时，因此在整个流程中应当让上面两行代码只执行一次。'''\n",
        "\n",
        "# target transform\n",
        "def LabelCorePa(ref, isd):\n",
        "    now = datetime.strptime(\"2022-01-01\", \"%Y-%M-%d\").year\n",
        "    years = now - datetime.strptime(isd, \"%Y-%M-%d\").year\n",
        "    score = ref/years       \n",
        "    label = int((score>0.5))\n",
        "    return label\n",
        "\n",
        "# 这应当视作代码的主体部分，暂时放在这里\n",
        "# def Txt2Vec(ttl):\n",
        "#     ttl = ttl.replace('\\n', '')\n",
        "\n",
        "#     input_text = ttl                                                            # 目前只用到了ttl\n",
        "#     inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
        "#     outputs = nlp_model(**inputs)\n",
        "#     last_hidden_states = outputs.last_hidden_state\n",
        "#     cls_vec = last_hidden_states[:,0,:].clone().detach().squeeze()\n",
        "\n",
        "#     return cls_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8tSi8TZFQyB"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, Subset, ConcatDataset, DataLoader\n",
        "from torch import nn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn import metrics\n",
        "\n",
        "import random\n",
        "\n",
        "# Dataset\n",
        "class PatDataset(Dataset):\n",
        "    def __init__(self, raw_data_path, transform = None, target_transform = LabelCorePa):\n",
        "        self.raw_data = pd.read_csv(raw_data_path)\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.raw_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ttl = self.raw_data.at[idx, \"patent_title\"]            \n",
        "        ref = self.raw_data.at[idx, \"patent_num_cited_by_us_patents\"]\n",
        "        isd = self.raw_data.at[idx, \"patent_date\"]\n",
        "        num_claims = self.raw_data.at[idx, \"patent_num_claims\"]\n",
        "        b_cits = self.raw_data.at[idx, \"patent_num_us_patent_citations\"]\n",
        "        inventors = self.raw_data.at[idx, \"inventors\"]\n",
        "        num_inventors = len(eval(inventors))\n",
        "        assignees = self.raw_data.at[idx, \"assignees\"]\n",
        "        if assignees == \"[{'assignee_sequence': None, 'assignee_key_id': None}]\":\n",
        "            num_assignees = 0\n",
        "        else:\n",
        "            num_assignees = len(eval(assignees))\n",
        "        IPCs = self.raw_data.at[idx, \"IPCs\"]\n",
        "        num_ipcs = len(eval(IPCs))\n",
        "        if self.transform:\n",
        "            text_vec = self.transform(ttl)\n",
        "        indexs = torch.tensor([num_claims, b_cits, num_inventors, num_assignees, num_ipcs], dtype=torch.float)\n",
        "        patent = indexs\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(ref, isd)\n",
        "            label = torch.tensor(label,dtype=torch.long)\n",
        "        return patent, label\n",
        "\n",
        "# Models\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, output_size),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "\n",
        "        # batch: 第几个batch；X: 包含batch_size个feature vec.\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        output = model(X).to(device)\n",
        "        loss = loss_fn(output, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 20 == 0:                                                     # train_size/batch_size = 100, 每20个batch输出一次结果，共输出5次。\n",
        "            loss, current = loss.item(), batch * len(X)                         \n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "# Test\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "            output = model(X)\n",
        "            # test_loss += loss_fn(output, y).item()\n",
        "            # correct += (output.argmax(1) == y).type(torch.float).sum().item()\n",
        "            \n",
        "            pred = output.argmax(1).cpu()\n",
        "            y = y.cpu()\n",
        "            if batch == 0:\n",
        "                Pred = pred\n",
        "                Y = y\n",
        "            else:\n",
        "                Pred = torch.cat((Pred, pred), dim = 0)\n",
        "                Y = torch.cat((Y, y), dim=0)\n",
        "\n",
        "    C_Mat = metrics.confusion_matrix(Y, Pred)\n",
        "    accuracy = metrics.accuracy_score(Y,Pred)\n",
        "    f1 = metrics.f1_score(Y, Pred)\n",
        "    recall = metrics.recall_score(Y, Pred)\n",
        "    precision = metrics.precision_score(Y, Pred)\n",
        "    print(C_Mat)\n",
        "    print(f\"acc:{accuracy:.4f}, f1:{f1:.4f}, recall:{recall:.4f}, prec:{precision:.4f}\")\n",
        "\n",
        "    # test_loss /= num_batches\n",
        "    # correct /= size\n",
        "    # print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgQRy866G5JW"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cnjn-MyeOvYq"
      },
      "outputs": [],
      "source": [
        "# HyperParams----Config.py\n",
        "input_size = 5\n",
        "hidden_size = 16\n",
        "output_size = 2\n",
        "\n",
        "learning_rate = 1e-3\n",
        "batch_size = 64                                                                 # test_size/batch_size = 25, 25 batches.\n",
        "epochs = 5\n",
        "\n",
        "num_train = 8000\n",
        "num_test = 2000\n",
        "\n",
        "# PIPLINE\n",
        "# DATA TO FIT A MODEL\n",
        "raw_data_path = \"T10k.csv\"\n",
        "dataset = PatDataset(raw_data_path)\n",
        "training_indices = [i for i in range(num_train)]\n",
        "test_indices = [i for i in range(num_train,num_train+num_test)]\n",
        "training_data_all = Subset(dataset, training_indices)\n",
        "test_data = Subset(dataset, test_indices)\n",
        "\n",
        "# Make pos:neg in training set 1:1\n",
        "pos_indices = []\n",
        "neg_indices = []\n",
        "for i in range(num_train):\n",
        "    if training_data_all[i][1] == 1:\n",
        "        pos_indices.append(i)\n",
        "    else:\n",
        "        neg_indices.append(i)\n",
        "num_pos = len(pos_indices)\n",
        "num_neg = len(neg_indices)\n",
        "print(f\"{num_neg}:{num_pos} = {num_neg/num_pos}\")\n",
        "neg_indices_sample = random.choices(neg_indices, k=num_pos)\t \n",
        "training_data_pos = Subset(training_data_all, pos_indices)\n",
        "training_data_neg = Subset(training_data_all, neg_indices_sample)\n",
        "training_data = ConcatDataset([training_data_pos, training_data_neg])\n",
        "print(\"Training sample number:\",len(training_data))\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZiFY_jDIhNQ"
      },
      "outputs": [],
      "source": [
        "# FIT A MODEL\n",
        "# PyTorch的逻辑是先初始化（喂超参），再进行函数计算（喂输入）\n",
        "model = SimpleNet(input_size, hidden_size, output_size).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(train_dataloader, model, loss_fn)                                 # performance on training data\n",
        "    test_loop(test_dataloader, model, loss_fn)                                  # performance on test data\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JT6lZqVrIsnt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtjHVuhNP3vn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNIQr9XuakxG"
      },
      "source": [
        "# Save&Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTO_IOiiarkm"
      },
      "outputs": [],
      "source": [
        "# 保存模型权重至当前文件夹\n",
        "torch.save(model.state_dict(), 'model_weights.pth')                             "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQejY8J8ay0T"
      },
      "outputs": [],
      "source": [
        "model = SimpleNet(input_size, hidden_size, output_size)                         # 需要是同一个模型\n",
        "model.load_state_dict(torch.load('model_weights.pth'))\n",
        "model.eval()\n",
        "\n",
        "'''\n",
        "be sure to call model.eval() method before inferencing to set the dropout and batch normalization layers to evaluation mode. \n",
        "Failing to do this will yield inconsistent inference results.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0CxBdcFdJkv"
      },
      "outputs": [],
      "source": [
        "# 直接保存/加载整个模型\n",
        "torch.save(model, 'model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HujEzt6d9k-"
      },
      "outputs": [],
      "source": [
        "model = torch.load('model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PISkr-lNd_iH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GSfZNi978Mu"
      },
      "source": [
        "# Discoveries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWQWhD2z8Bfi"
      },
      "source": [
        "事实证明，只用ttl几乎相当于没有给模型提供有用信息，模型倾向于只预测其中一类。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKJ3OC1k7-al"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "fKdAJ8l5F_yg",
        "F_Rn1K8pE3Z0",
        "DgQRy866G5JW",
        "VNIQr9XuakxG",
        "6GSfZNi978Mu"
      ],
      "name": "P0_v05.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.11 ('DataAnalysis')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "c18f0d304bfd4ea88d1106146a85daf3cb92cb10206ff0ef630b4b8ca37137ad"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
